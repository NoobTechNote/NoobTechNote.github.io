---
title: "Ch11: 虛擬化容器"
tsidebar_label: "Ch11: 虛擬化容器"
sidebar_position: 11
---

# 虛擬化容器

:::tip

- 讓軟件能夠在任何環境、任何物理機器上達到“一次編譯，到處運行”，這並不是一個簡單的目標
- 一個計算機軟件要能夠正確運行，需要有以下三方面的兼容性來共同保障

:::

- **ISA 兼容**: 機器指令集兼容性
- **ABI 兼容**: 系統或者依賴庫的二進制兼容性
- **環境兼容**: 環境的兼容性

### 虛擬化技術

- 指令集虛擬化（ISA Level Virtualization）
- 硬件抽象層虛擬化（Hardware Abstraction Level Virtualization）
- 操作系統層虛擬化（OS Level Virtualization）
- 運行庫虛擬化（Library Level Virtualization）
- 語言層虛擬化（Programming Language Level Virtualization）

## 容器的崛起

### 隔離文件：chroot

:::note

- Version 7 UNIX 系統中提供的 `chroot` 命令
- Linux Kernel 2.3.41 版內核引入了 `pivot_root`

:::

- 當某個進程經過 `chroot` 操作之後，它的根目錄就會被鎖定在命令參數所指定的位置，以後它或者它的子進程將不能再訪問和操作該目錄之外的其他文件。
- `pivot_root` 技術來實現文件隔離，`pivot_root` 直接切換了根文件系統（rootfs），有效地避免了 `chroot` 命令可能出現的安全性漏洞

### 隔離訪問：namespaces

:::note

- Linux Kernel 2.4.19 版內核引入了一種全新的隔離機制：Linux 名稱空間（Linux Namespaces）

:::

- 一種由內核直接提供的全局資源封裝，是內核針對進程設計的訪問隔離機制。

#### Linux 名稱空間支持以下八種資源的隔離

| 名稱空間 | 隔離內容                                                                                                                                             | 內核版本 |
| -------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| Mount    | 隔離文件系統，功能上大致可以類比 chroot                                                                                                              | 2.4.19   |
| UTS      | 隔離主機的 Hostname、Domain names                                                                                                                    | 2.6.19   |
| IPC      | 隔離進程間通信的渠道（詳見“遠程服務調用”中對 IPC 的介紹）                                                                                            | 2.6.19   |
| PID      | 隔離進程編號，無法看到其他名稱空間中的 PID，意味著無法對其他進程產生影響                                                                             | 2.6.24   |
| Network  | 隔離網絡資源，如網卡、網絡棧、IP 地址、端口，等等                                                                                                    | 2.6.29   |
| User     | 隔離用戶和用戶組                                                                                                                                     | 3.8      |
| Cgroup   | 隔離 cgroups 信息，進程有自己的 cgroups 的根目錄視圖（在/proc/self/cgroup 不會看到整個系統的信息）。cgroups 的話題很重要，稍後筆者會安排一整節來介紹 | 4.6      |
| Time     | 隔離系統時間，2020 年 3 月最新的 5.6 內核開始支持進程獨立設置系統時間                                                                                | 5.6      |

:::tip
syslog 時至今日還沒被隔離，容器內可以看到容器外其他進程產生的內核 syslog
:::

### 隔離資源：cgroups

> Control Groups，目前常用的簡寫為 `cgroups`

- 與名稱空間(namespaces)一樣都是直接由內核提供的功能
- 隔離或者說分配並限制某個進程組能夠使用的資源配額

#### Linux 控制群組子系統

| 控制組子系統 | 功能                                                                                       |
| ------------ | ------------------------------------------------------------------------------------------ |
| blkio        | 為塊設備（如磁盤，固態硬盤，USB 等等）設定 I/O 限額。                                      |
| cpu          | 控制 cgroups 中進程的處理器佔用比率。                                                      |
| cpuacct      | 自動生成 cgroups 中進程所使用的處理器時間的報告。                                          |
| cpuset       | 為 cgroups 中的進程分配獨立的處理器（包括多路系統的處理器，多核系統的處理器核心）。        |
| devices      | 設置 cgroups 中的進程訪問某個設備的權限（讀、寫、創建三種權限）。                          |
| freezer      | 掛起或者恢復 cgroups 中的進程。                                                            |
| memory       | 設定 cgroups 中進程使用內存的限制，並自動生成內存資源使用報告。                            |
| net_cls      | 使用等級識別符標記網絡數據包，可允許 Linux 流量控製程序識別從具體 cgroups 中生成的數據包。 |
| net_prio     | 用來設置網絡流量的優先級。                                                                 |
| hugetlb      | 主要針對於 HugeTLB 系統進行限制。                                                          |
| perf_event   | 允許 Perf 工具基於 cgroups 分組做性能監測。                                                |

- 2008 年合併到 2.6.24 版的內核後正式對外發布，這一階段的 cgroups 被稱為“第一代 `cgroups`”
- 2016 年 3 月發布的 Linux Kernel 4.5 中，搭載了由 Facebook 工程師（主要是 Tejun Heo）重新編寫的“第二代 `cgroups`”

:::tip
目前這兩個版本的 cgroups 在 Linux 內核代碼中是並存的，稍後介紹的 Docker 暫時僅支持第一代的 cgroups。
:::

### 封裝系統：LXC

:::note
為降低普通用戶綜合使用 `namespaces`、`cgroups` 這些低級特性的門檻，2008 年 Linux Kernel 2.6.24 內核剛剛開始提供 `cgroups` 的同一時間，就馬上發布了名為 Linux 容器（LinuX Containers，LXC）的系統級虛擬化功能。
:::

### 封裝應用：Docker

:::note

> Solomon Hykes，[Stackoverflow](https://stackoverflow.com/questions/17989306/what-does-docker-add-to-lxc-tools-the-userspace-lxc-tools/)，2013

**為什麼要用 Docker 而不是 LXC？（Why would I use Docker over plain LXC？）**

Docker 除了包裝來自 Linux 內核的特性之外，它的價值還在於：

- **跨機器的綠色部署**: Docker 定義了一種將應用及其所有的環境依賴都打包到一起的格式，彷彿它原本就是綠色軟件一樣。LXC 並沒有提供這樣的能力，使用 LXC 部署的新機器很多細節都依賴人的介入，虛擬機的環境幾乎肯定會跟你原本部署程序的機器有所差別。
- **以應用為中心的封裝**: Docker 封裝應用而非封裝機器的理念貫穿了它的設計、API、界面、文檔等多個方面。相比之下，LXC 將容器視為對系統的封裝，這局限了容器的發展。
- **自動構建**: Docker 提供了開發人員從在容器中構建產品的全部支持，開發人員無需關注目標機器的具體配置，即可使用任意的構建工具鏈，在容器中自動構建出最終產品。
- **多版本支持**: Docker 支持像 Git 一樣管理容器的連續版本，進行檢查版本間差異、提交或者回滾等操作。從歷史記錄中你可以查看到該容器是如何一步一步構建成的，並且只增量上傳或下載新版本中變更的部分。
- **組件重用**: Docker 允許將任何現有容器作為基礎鏡像來使用，以此構建出更加專業的鏡像。
- **共享**: Docker 擁有公共的鏡像倉庫，成千上萬的 Docker 用戶在上面上傳了自己的鏡像，同時也使用他人上傳的鏡像。
- **工俱生態**: Docker 開放了一套可自動化和自行擴展的接口，在此之上，還有很多工具來擴展其功能，譬如容器編排、管理界面、持續集成等等。

:::

#### 開放容器交互標準

- 運行時標準（runtime-spec）: 運行時標准定義了應該如何運行一個容器、如何管理容器的狀態和生命週期、如何使用操作系統的底層特性（namespaces、cgroup、pivot_root 等）
- 容器鏡像標準（image-spec）: 容器鏡像標準規定了容器鏡像的格式、配置、元數據的格式，可以理解為對鏡像的靜態描述
- 鏡像分發標準（distribution-spec，分發標準還未正式發布）: 鏡像分發標準則規定了鏡像推送和拉取的網絡交互過程。

#### Docker、containerd 和 runC 的交互關係

![Docker、containerd 和 runC 的交互關係](./ch11/11-2.png)

### 封裝集群：Kubernetes

#### Kubernetes 與容器引擎的調用關係

![Kubernetes 與容器引擎的調用關係](./ch11/11-3.png)

將這個階段 Kubernetes 與容器引擎的調用關係捋直，並結合上一節提到的 Docker 捐獻 containerd 與 runC 後重構的調用，完整的調用鏈條如下所示：

> **Kubernetes Master → kubelet → DockerManager → Docker Engine → containerd → runC**

Docker 對 Kubernetes 來說只是一項默認依賴，而非之前的無可或缺了，它們的調用鍊為：

> **Kubernetes Master → kubelet → KubeGenericRuntimeManager → DockerShim → Docker Engine → containerd → runC**

開源版 Kubernetes 是使用 CRI-O、cri-containerd 抑或是 DockerShim 作為 CRI 實現，完全可以由用戶自由選擇（根據用戶宿主機的環境選擇），但在 RedHat 自己擴展定制的 Kubernetes 企業版，即 OpenShift 4 中，調用鏈已經沒有了 Docker Engine 的身影：

> **Kubernetes Master → kubelet → KubeGenericRuntimeManager → CRI-O→ runC**

Kubernetes 從 1.10 版本宣布開始支持 containerd 1.1，在調用鏈中已經能夠完全抹去 Docker Engine 的存在：

> **Kubernetes Master → kubelet → KubeGenericRuntimeManager → containerd → runC**

## 以容器構建系統

> 容器之間順暢地交互通信是協作的核心需求，但容器協作並不僅僅是將容器以高速網絡互相連接而已。如何調度容器，如何分配資源，如何擴縮規模，如何最大限度地接管系統中的非功能特性，讓業務系統盡可能免受分佈式複雜性的困擾都是容器編排框架必須考慮的問題，只有恰當解決了這一系列問題，雲原生應用才有可能獲得比傳統應用更高的生產力。

### 隔離與協作

對親密協作的容器，是指它們被調度到同一個集群節點上，可以通過共享本地磁盤等方式協作；而超親密的協作是特指多個容器位於同一個 Pod 這種特殊關係，它們將默認共享：

- **UTS 名稱空間**：所有容器都有相同的主機名和域名
- **網絡名稱空間**：所有容器都共享一樣的網卡、網絡棧、IP 地址，等等。因此，同一個 Pod 中不同容器佔用的端口不能衝突
- **IPC 名稱空間**：所有容器都可以通過信號量或者 POSIX 共享內存等方式通信
- **時間名稱空間**：所有容器都共享相同的系統時間

#### Pod

> 同一個 Pod 的容器，只有 PID 名稱空間和文件名稱空間默認是隔離的。但通過 Pod 元數據定義中的 spec.shareProcessNamespace 來改變這點。共享 PID 名稱空間，容器封裝的應用進程就不再具有 PID 為 1 的特徵了。

Pod 是隔離與調度的基本單位，也是我們接觸的第一種 Kubernetes 資源。Kubernetes 將一切皆視為資源，不同資源之間依靠層級關係相互組合協作，這個思想是貫穿 Kubernetes 整個系統的兩大核心設計理念之一，不僅在容器、Pod、主機、集群等計算資源上是這樣，在工作負載、持久存儲、網絡策略、身份權限等其他領域中也都有著一致的體現。

![Kubernetes 的計算資源](./ch11/11-4.png)

#### Kubernetes 的計算資源

- **容器（Container）**: 延續了自 Docker 以來一個容器封裝一個應用進程的理念，是鏡像管理的最小單位。
- **生產任務（Pod）**: 補充了容器化後缺失的與進程組對應的“容器組”的概念，Pod 中容器共享 UTS、IPC、網絡等名稱空間，是資源調度的最小單位。
- **節點（Node）**: 對應於集群中的單台機器，這裡的機器即可以是生產環境中的物理機，也可以是雲計算環境中的虛擬節點，節點是處理器和內存等資源的資源池，是硬件單元的最小單位。
- **集群（Cluster）**: 對應於整個集群，Kubernetes 提倡理念是面向集群來管理應用。當你要部署應用的時候，只需要通過聲明式 API 將你的意圖寫成一份元數據（Manifests），將它提交給集群即可，而無需關心它具體分配到哪個節點（儘管通過標籤選擇器完全可以控制它分配到哪個節點，但一般不需要這樣做）、如何實現 Pod 間通信、如何保證韌性與彈性，等等，所以集群是處理元數據的最小單位。
- **集群聯邦（Federation）**: 對應於多個集群，通過聯邦可以統一管理多個 Kubernetes 集群，聯邦的一種常見應用是支持跨可用區域多活、跨地域容災的需求。

### 韌性與彈性

- 架構師專門設計了[服務容錯](./ch8#服務容錯)的策略和模式，Kubernetes 作為雲原生時代的基礎設施，也盡力幫助程序員以最小的代價來實現容錯，為系統健壯運行提供底層支持。

以房間中空調自動調節溫度為例子介紹了控制迴路的一般工作過程的：當你設置好了溫度，就是告訴空調你對溫度的“期望狀態”（Desired State），而傳感器測量出的房間實際溫度是“當前狀態”（Current State）。根據當前狀態與期望狀態的差距，控制器對空調製冷的開關進行調節控制，就能讓其當前狀態逐漸接近期望狀態。

![控制迴路](./ch11/11-5.png)

- 通過描述清楚這些資源的期望狀態，由 Kubernetes 中對應監視這些資源的控制器來驅動資源的實際狀態逐漸向期望狀態靠攏
- 在 Kubernetes 元數據文件中的 `spec` 字段所描述的便是資源的期望狀態

#### ReplicaSet

ReplicaSet 本身可以保證 Pod 出現故障時自動恢復，但是在升級程序版本時，ReplicaSet 不得不主動中斷舊 Pod 的運行，重新創建新版的 Pod，這會造成服務中斷。

#### Deployment

由 Deployment 來創建 ReplicaSet，再由 ReplicaSet 來創建 Pod，當你更新 Deployment 中的信息（譬如更新了鏡像的版本）以後，部署控制器就會跟踪到你新的期望狀態，自動地創建新 ReplicaSet，並逐漸縮減舊的 ReplicaSet 的副本數，直至升級完成後徹底刪除掉舊 ReplicaSet

![Deployment 滾動更新過程](./ch11/11-6.png)

#### Autoscaling

Kubernetes 提供了 Autoscaling 資源和自動擴縮控制器，能夠自動根據度量指標，如處理器、內存佔用率、用戶自定義的度量值等，來設置 Deployment（或者 ReplicaSet）的期望狀態，實現當度量指標出現變化時，系統自動按照“Autoscaling → Deployment → ReplicaSet → Pod”這樣的順序層層變更，最終實現根據度量指標自動擴容縮容。

:::info
故障恢復、滾動更新、自動擴縮這些特性，在雲原生中時代裡常被概括成服務的彈性（Elasticity）與韌性（Resilience），ReplicaSet、Deployment、Autoscaling 的用法，也屬於是所有 Kubernetes 教材資料都會講到的“基礎必修課”
:::

