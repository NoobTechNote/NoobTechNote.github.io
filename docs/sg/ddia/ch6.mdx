---
title: "Ch6: 分區"
sidebar_label: "Ch6: 分區"
sidebar_position: 6
---

分區又稱 partition or sharding，將大型數據庫分解成小型數據庫的方式。

- MongoDB, Elasticsearch, Solr Cloud - shard
- HBase - Region
- Bigtable - tablet
- Cassandra, Riak - vnode
- Couchbase - vBucket

分區的特性:

- 可擴展性
- 資料可以分佈在不共享集群的不同節點上
- 查詢負載可以分佈在不同節點上

本章分塊:

- 索引與分區
- 重新平衡分區
- 數據庫如何將請求路由到正確的分區並執行查詢

## 分區與複製

分區被複製到多個節點，並採取主從複製模型，每個節點可能是某些 分區的領導者，同時是其他分區的追隨者，
以提供容錯性。

![](./ch6/fig6_1.png)

### 鍵值數據的分區

分區的理想下，查詢負載會平均分佈在所有節點上，而不是集中在少數節點上。
但現實情況是，數據集中或是常被使用在某個分區(hot spot)，對整體而言是偏斜的(skew)。
假如讓數據平均分配到不同的節點，解決偏斜的問題，但要讀取數據時，需要並行查詢所有節點，增加了查詢的成本。
為了可以快速找到資料，儲存資料時可以使用鍵值的方式建立建立數據庫，通過主鍵查詢，可以直接定位到正確的節點。
ex: 百科全書目錄

#### 根據鍵得範圍分區

- 每個分區有鍵範圍，鍵是有序的，範圍內的鍵值都屬於這個分區，可以輕易知道資料在哪個分區內進而搜尋。
- 為了均勻分配數據，分區邊界需要依據數據調整，分區邊界可以由管理員手動選擇，也可以由數據庫自動選擇。
- 容易進行範圍掃描，會去範圍內多個相關紀錄。
- 主鍵太單一的話容易造成熱點。 ex: 如果都搜尋今天的數據，會造成熱點，需要改用其他資料作為主鍵。

![](./ch6/fig6_2.png)

#### 根據鍵的散列分區

- 使用散列函數來給鍵值分配分區，這樣可以均勻分配數據到不同的節點。ex: Fowler-Noll-Vo
- 失去範圍查詢的能力。

![](./ch6/fig6_3.png)

> 許多語言內建的 hash function 並不適合作為函數，因為同個輸入可能不會有相同的 hash 值。

相關的服務:

- Riak
- Couchbase
- Voldemort
- Cassandra 可以使用複合主鍵健行範圍掃描。

##### 負載傾斜與消除熱點

雖然散列分區可以均勻分配數據，但是有可能會有熱點問題。ex: 名人的 user Id。
因為 hash 之後的值是相同的，書中的方法是在主鍵加上兩位數的隨機數，這樣可以讓數據分散到不同的節點，但還需要但還需要額外工作合併結果。

## 分片與次級索引

次級索引通常並不能唯 一地標識記錄，而是一種搜索記錄中出現特定值的方式。
尋找用戶 123 的所有操作，查找包含 hogwash 的所有文章，尋找所有顏色為紅色的車輛等等。
次級索引的問題是它們不能整齊地映射到分區(?)。有兩種用次級索引對數據庫進行分區的方法:

- 基於文檔的分區(document-based)
- 基於關鍵詞(term-based)的分區

相關的服務:

- Riak
- Solr
- Elasticsearch

### 按文檔的次級索引

範例中，一個二手車網站使用 ID 作文檔的分區，但是次級索引是根據車輛的顏色。
想要查找紅色的車輛，需要先查詢次級索引，找到所有紅色車輛的 ID，再去主索引查詢詳細資料。
因為只需要處理 ID 的分區，文檔分區又稱本地索引(local index)。

由於資料分散在各個分區，所以需要查詢各個分區並合併結果(scatter/gather)，所以次級索引的成本是昂貴的，而且尾部延遲也會增加。

![](./ch6/fig6_4.png)

相關的服務:

- MongoDB
- Riak
- Cassandra
- Elasticsearch
- SolrCloud
- VoltDB

### 根據關鍵值的次級索引

相較於文檔索引(本地索引)在每個分區建立屬於自己的次級索引，關鍵值索引 term-partitioned(全局索引)橫跨所有分區建立全局的次級索引。
範例中，所有紅色車子的 ID 都在 Partition 0，製造商的名字也依照自首，分類在不同分區。
![](./ch6/fig6_5.png)

- 範圍掃描
- 因為對關鍵詞進行 hash 分區，所以提供附載均衡。
- 讀取比起文檔索引更快。
- 寫入速度較慢，因為需要更新全局索引。

#### 分區再平衡

隨著時間的推移，數據庫會有各種變化。

- 查詢吞吐量增加，所以您想要添加更多的 CPU 來處理負載。
- 數據集大小增加，所以您想添加更多的磁盤和 RAM 來存儲它。
- 機器出現故障，其他機器需要接管故障機器的責任。

所有這些更改都需要數據和請求從一個節點移動到另一個節點。
將負載從集群中的一個節點 向另一個節點移動的過程稱為再平衡(reblancing)。
無論使用哪種分區方案，再平衡通常都要滿足一些最低要求:

- 再平衡之後，負載(數據存儲，讀取和寫入請求)應該在集群中的節點之間公平地共享。
- 再平衡發生時，數據庫應該繼續接受讀取和寫入。
- 節點之間只移動必須的數據，以便快速再平衡，並減少網路和磁碟 I/O 負載。

### 平衡策略

#### 反面教材: hash mode N

幾如果們有 10 個節點 N，將資料的 ID 用 mod 取餘數，分配到第 N 個節點上，這樣會造成當節點數量改變時，所有的資料都需要重新分配。

#### 固定數量的分區

分區的數量是固定的，而且被平均分配到節點上，如果增加節點，會把各個節點的一些分區分配到新的節點上，
相反的，如果減少節點，會把被減少的節點上的分區分配到其他節點上。
這種變更並不是即時的 — 在網絡上傳輸大量的數據需要一些時間 — 所以在傳輸過程中，原有分區仍然會接受讀寫操作。
![](./ch6/fig6_6.png)

- 可以透過增加性能更好的節點汰換舊的節點，或是分配更多的分區到性能較好的節點上。
- 分區數量一開始就要決定好，但不好預估。
- 分區太大，再平衡跟節點故障恢復的成本變高，分區太小，會造成分區數量過多，增加管理成本。

#### 動態分區

鍵範圍分區的數據庫，因為分區有固定範圍和固定數量，如果出現邊界錯誤，可能會導致一個分區中的所有數據或者其他分區中的所有數據為空，需要再手動調整邊界。
一些按鍵的範圍進行分區的數據庫(HBase, RethinkDB)，會動態創建分區:

- 當分區超過配置的大小時，會自動分割分區並分配到其他節點上。
- 當分區太小時，會合併分區到其他節點上。
- 分區的數量依照數據的總量來動態增加減少。

#### 按節點比例分區

#### 运维:手动还是自动平衡

### 请求路由

#### 执行并行查询

### Summary
